{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4234b82",
   "metadata": {},
   "source": [
    "# Program Description: Statistics of Structure Descriptors (Module 5)\n",
    "\n",
    "## Overview:\n",
    "This module performs statistical analysis on structural descriptors obtained from previous modules. It generates various statistical tables and visualizations, including histograms, density distribution graphs, and box plots. The statistical analysis is applied to features like coordination numbers (CN), bond lengths (CR), and other structure-related properties.\n",
    "\n",
    "The module can also analyze datasets after optimization and data division, with the option to exclude outlier samples based on predefined criteria. This is controlled by parameters within the module.\n",
    "\n",
    "## Input Files/Folders:\n",
    "- **Dataset Files**: \n",
    "  - These are the files containing the structural descriptors, typically located in the folder where this module is executed (by default).\n",
    "  - The files contain features such as `chi`, `xmu`, `rdf`, CN, CR, and other descriptors.\n",
    "\n",
    "## Output Files/Folders:\n",
    "- **Output Folder**: \n",
    "  - A new folder named `statistics_{current_time}` is created in the input folder, where `{current_time}` is the current timestamp.\n",
    "  - This folder will contain statistical analysis tables and visualizations (e.g., histograms, density distribution graphs, box plots) for the structural descriptors.\n",
    "\n",
    "## Parameters:\n",
    "- **remove_excluded**: \n",
    "  - **True**: Excludes outlier files from the statistical analysis, using predefined criteria for outlier detection.\n",
    "  - **False**: Includes all files in the statistical analysis, regardless of whether they are identified as outliers or not.\n",
    "  \n",
    "- **read_after_division**: \n",
    "  - **True**: Processes the dataset after it has been divided into different sets (e.g., training, validation, test).\n",
    "  - **False**: Processes the entire dataset, without considering any division.\n",
    "  - Note: The `remove_excluded` parameter is only relevant for datasets before division. If analyzing data after division, this parameter can be ignored.\n",
    "\n",
    "## Process:\n",
    "1. **Statistical Analysis**: \n",
    "   - The module computes basic statistical metrics, such as mean, median, standard deviation, and variance, for each structural descriptor.\n",
    "   - It visualizes these metrics with the help of histograms, density distribution plots, and box plots, making it easy to interpret the distribution and spread of the data.\n",
    "  \n",
    "2. **Outlier Handling**:\n",
    "   - The module can exclude outlier data points from the analysis based on user-defined criteria, controlled via the `remove_excluded` parameter.\n",
    "   - If `remove_excluded` is set to **True**, outliers identified by an external process (like Module 7) will be removed before analysis.\n",
    "\n",
    "3. **Data Division Handling**:\n",
    "   - The `read_after_division` parameter allows the user to specify whether to process the dataset before or after it has been divided into subgroups (e.g., training, validation, test).\n",
    "   - If **True**, the program processes only the datasets that have been divided, ensuring that the statistical analysis is performed separately on each subset.\n",
    "\n",
    "4. **Output Generation**:\n",
    "   - The results of the statistical analysis are stored in the `statistics_{current_time}` directory, including CSV files with summary statistics and image files for the generated plots (e.g., histograms, density plots, and box plots).\n",
    "\n",
    "## Example Workflow:\n",
    "1. **Input**: Dataset files containing structural descriptors (e.g., `chi`, `xmu`, `rdf`, CN, CR).\n",
    "2. **Process**: Perform statistical analysis (mean, median, standard deviation), generate visualizations (histograms, density plots, box plots), and exclude outliers if needed.\n",
    "3. **Output**: Statistical tables and plots saved in a folder named `statistics_{current_time}`.\n",
    "\n",
    "## Notes:\n",
    "- This module helps users better understand the statistical properties of the structural descriptors and their distributions across samples.\n",
    "- The `remove_excluded` parameter allows flexibility in handling outliers, making the analysis more robust when needed.\n",
    "- The visualizations generated (histograms, box plots, density plots) provide a clear overview of the data's spread and help in identifying patterns or discrepancies in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2d9a0",
   "metadata": {},
   "source": [
    "contacts: zhaohf@ihep.ac.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e75dc4",
   "metadata": {},
   "source": [
    "#  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f4c031",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:41.531888Z",
     "start_time": "2025-01-21T01:56:40.662930Z"
    }
   },
   "outputs": [],
   "source": [
    "from os.path import join, splitext, basename\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys \n",
    "from datetime import datetime\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211918e6",
   "metadata": {},
   "source": [
    " ## Version Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c778cf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:41.540066Z",
     "start_time": "2025-01-21T01:56:41.533942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib: 3.7.5\n",
      "pandas: 2.0.3\n",
      "seaborn: 0.13.2\n",
      "numpy: 1.23.5\n",
      "sklearn: 1.3.2\n",
      "scipy: 1.10.1\n",
      "Python: 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:46:39) \n",
      "[GCC 10.4.0]\n"
     ]
    }
   ],
   "source": [
    "def get_python_version():\n",
    "    return sys.version\n",
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        version = getattr(module, '__version__', None)\n",
    "        if version:\n",
    "            return version\n",
    "        else:\n",
    "            return pkg_resources.get_distribution(package_name).version\n",
    "    except (ImportError, AttributeError, pkg_resources.DistributionNotFound):\n",
    "        return \"Version info not found\"\n",
    "\n",
    "packages = ['matplotlib', 'pandas', 'seaborn','numpy','sklearn','scipy']\n",
    "for package in packages:\n",
    "    print(f\"{package}: {get_package_version(package)}\")\n",
    "print(f\"Python: {get_python_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d411e",
   "metadata": {},
   "source": [
    "# Parameter Settings\n",
    "\n",
    "## Input File:\n",
    "- **load_path**: \n",
    "  - This parameter specifies the path to the input dataset file(s) containing structural descriptors (e.g., `chi`, `xmu`, `rdf`, CN, CR). \n",
    "  - The default path is set to the folder where the module is located, but users can modify this to point to any location containing the dataset.\n",
    "\n",
    "## Output:\n",
    "- **Output Directory**: \n",
    "  - The processed statistical results will be saved in a directory named `statistics_{current_time}` within the input folder. \n",
    "  - `{current_time}` is a timestamp of when the analysis was performed, ensuring the output is uniquely labeled.\n",
    "\n",
    "## Labels Parameter:\n",
    "- **labels**:\n",
    "  - Defines which structural descriptors to analyze. It can be one or more of the following:\n",
    "    - `cr`: Bond length (CR) data.\n",
    "    - `cn`: Coordination number (CN) data.\n",
    "  - The range of values for this parameter is `[cr, cn]`. You can select both descriptors or a single one to perform statistical analysis.\n",
    "\n",
    "## Precision Parameters:\n",
    "- **cn_precision**:\n",
    "  - Specifies the number of decimal places to retain for the statistical analysis of the **coordination number (CN)** data. \n",
    "  - This allows users to control the precision level of CN values in the output statistics (e.g., 2 decimal places for CN precision).\n",
    "  \n",
    "- **cr_precision**:\n",
    "  - Specifies the number of decimal places to retain for the statistical analysis of the **bond length (CR)** data. \n",
    "  - Similar to `cn_precision`, this parameter controls the precision level of CR values in the output statistics.\n",
    "\n",
    "## Notes:\n",
    "- **load_path** should point to the correct dataset folder containing the structural descriptors.\n",
    "- The **labels** parameter determines which descriptors will be analyzed (CN and/or CR).\n",
    "- **cn_precision** and **cr_precision** control the decimal precision in the statistical output for CN and CR, respectively.\n",
    "- The output is saved in a timestamped folder for easy versioning and reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40a0058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:41.549875Z",
     "start_time": "2025-01-21T01:56:41.541200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '0926-datasets/prepare/indices_to_move_JmolNN.csv' not found. Unable to process the filtered dataset.\n"
     ]
    }
   ],
   "source": [
    "# Range of parameters for coordination number (`cn`) and coordination bond length (`cr`)\n",
    "labels = [\"cr\", \"cn\"]\n",
    "\n",
    "# `read_after_division` determines whether to use the dataset before or after statistical division\n",
    "read_after_division = False\n",
    "\n",
    "# `cn_precision` is the precision limit for the coordination number. Although the coordination number is theoretically an integer (typically between 1 and 12),\n",
    "# it can sometimes have decimal values in chemistry.\n",
    "# `cr_precision` is the precision limit for the coordination bond length (in angstroms). \n",
    "# The first coordination bond length typically ranges between 2 and 3 angstroms, and XAS measurement precision is typically around 0.01 angstroms.\n",
    "cn_precision = 0\n",
    "cr_precision = 1\n",
    "\n",
    "# The `remove_excluded` parameter determines whether to process the filtered data.\n",
    "# It can be set to `True` or `False`.\n",
    "remove_excluded = True\n",
    "\n",
    "# `keyword` defines the search term for identifying relevant data. Possible values can be elements, compounds, or other identifiers (e.g., \"Cu\").\n",
    "keyword = \"Cu\"\n",
    "\n",
    "# Get the current timestamp for file naming and logging\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "# If `read_after_division` is True, load the dataset after it has been divided and perform relevant operations\n",
    "if read_after_division:\n",
    "    dateset_path= \"0926-datasets\"\n",
    "    load_path = os.path.join(dateset_path, \"prepare\")\n",
    "    date_path = os.path.join(load_path, \"datasets(JmolNN)\")  # Specify the directory for JmolNN dataset\n",
    "    statistics_path = os.path.join(dateset_path, f\"division_statistics_{current_time}\")\n",
    "    output_log_path = os.path.join(statistics_path, 'output_log.txt')\n",
    "    os.makedirs(statistics_path, exist_ok=True)\n",
    "    # Uncomment and select from these if analyzing specific dataset subsets: [\"all\", \"train\", \"valid\", \"test\"]\n",
    "    # If analyzing the divided dataset, select the appropriate structure data method\n",
    "    method = \"JmolNN\"\n",
    "    # Specify the dataset(s) to be counted (all, train, etc.)\n",
    "    data_set = [\"all\", \"train\"]\n",
    "else:\n",
    "    # If `read_after_division` is False, load the dataset before division\n",
    "    dateset_path= \"0926-datasets\"\n",
    "    load_path = os.path.join(dateset_path, \"prepare\")\n",
    "    statistics_path = os.path.join(dateset_path, f\"statistics_{current_time}\")\n",
    "    output_log_path = os.path.join(statistics_path, 'output_log.txt')\n",
    "    os.makedirs(statistics_path, exist_ok=True)\n",
    "    # If analyzing the filtered dataset, select the appropriate method to find the corresponding index file\n",
    "    method = \"JmolNN\"\n",
    "    \n",
    "    if remove_excluded:\n",
    "        # `excluded_indices_file` contains indices of the excluded samples that need to be removed from the dataset\n",
    "        excluded_indices_file = os.path.join(load_path, f\"indices_to_move_{method}.csv\")\n",
    "        excluded_indices = []\n",
    "        \n",
    "        # If the excluded indices file exists, load the indices to be excluded\n",
    "        if os.path.exists(excluded_indices_file):\n",
    "            excluded_indices = pd.read_csv(excluded_indices_file)[\"index\"].tolist()\n",
    "            print(f\"Read {len(excluded_indices)} excluded sample indices. Processing the filtered dataset.\")\n",
    "            logging.info(f\"Read {len(excluded_indices)} excluded sample indices. Processing the filtered dataset.\")\n",
    "        else:\n",
    "            if remove_excluded:\n",
    "                print(f\"File '{excluded_indices_file}' not found. Unable to process the filtered dataset.\")\n",
    "            else:\n",
    "                print(f\"File '{excluded_indices_file}' not found. Processing the entire dataset without filtering.\")\n",
    "    else:\n",
    "        # If no excluded samples need to be removed, process the entire dataset\n",
    "        excluded_indices = []\n",
    "        print(f\"Read {len(excluded_indices)} excluded sample indices. Processing the entire dataset.\")\n",
    "        logging.info(f\"Read {len(excluded_indices)} excluded sample indices. Processing the entire dataset.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd6d3c",
   "metadata": {},
   "source": [
    "## Fix: Check if the file has any issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db4e4c00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:41.559477Z",
     "start_time": "2025-01-21T01:56:41.551351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0926-datasets/statistics_20250121_0956' already exists, no need to create.\n"
     ]
    }
   ],
   "source": [
    "# Function to build file paths for labels based on the given base path, keyword, label types, and data subsets\n",
    "def build_label_paths(base_path, keyword, labels, data_subsets):\n",
    "    file_paths = {}\n",
    "    for label_type in labels:\n",
    "        for subset in data_subsets:\n",
    "            # Define the pattern to search for label files based on the label type and data subset\n",
    "            pattern = os.path.join(base_path, f\"*label_{label_type}_{subset}*.txt\")\n",
    "            print(f\"Checking pattern: {base_path}/*label_{label_type}_{subset}*.txt\")\n",
    "            # Use glob to find all files matching the pattern\n",
    "            matched_files = glob.glob(pattern)\n",
    "            \n",
    "            # If files are matched, store the first matched file in the dictionary\n",
    "            if matched_files:\n",
    "                print(f\"Matched files: {matched_files}\")\n",
    "                key = f\"{keyword}_{label_type}_{subset}\"\n",
    "                file_paths[key] = matched_files[0]\n",
    "            else:\n",
    "                # If no files are matched, log a warning\n",
    "                logging.warning(f\"No files matched for pattern: {pattern}\")\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "# Depending on the `read_after_division` flag, build the appropriate file paths\n",
    "if read_after_division:\n",
    "    # If reading after division, use `build_label_paths` to get paths for the divided data\n",
    "    label_paths = build_label_paths(date_path, keyword, labels, data_set)\n",
    "else:\n",
    "    # Otherwise, create a dictionary of paths for the labels in the base load path\n",
    "    paths = {label: os.path.join(load_path, label) for label in labels}\n",
    "\n",
    "# Ensure that the directory for statistics exists (create it if not)\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        # If the directory does not exist, create it\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        # If the directory already exists, no need to create it\n",
    "        print(f\"Directory '{directory}' already exists, no need to create.\")\n",
    "\n",
    "# Ensure that the directory for storing statistics exists\n",
    "ensure_directory_exists(statistics_path)\n",
    "\n",
    "# Set up logging for the output with the specified log file path and format\n",
    "logging.basicConfig(filename=output_log_path, level=logging.INFO, format='%(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b7a9ce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:41.568736Z",
     "start_time": "2025-01-21T01:56:41.560781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cr: 0926-datasets/prepare/cr\n",
      "cn: 0926-datasets/prepare/cn\n",
      "Directory '0926-datasets/statistics_20250121_0956' already exists, no need to create.\n"
     ]
    }
   ],
   "source": [
    "# Function to build file paths for labels based on the given base path, keyword, labels, and data subsets\n",
    "def build_label_paths(base_path, keyword, labels, data_subsets):\n",
    "    file_paths = {}\n",
    "    for label_type in labels:\n",
    "        for subset in data_subsets:\n",
    "            # Define the pattern to search for label files based on the label type and data subset\n",
    "            pattern = os.path.join(base_path, f\"*label_{label_type}_{subset}*.txt\")\n",
    "            print(f\"Checking pattern: {base_path}/*label_{label_type}_{subset}*.txt\")\n",
    "            # Use glob to find all files matching the pattern\n",
    "            matched_files = glob.glob(pattern)\n",
    "            \n",
    "            # If files are matched, store the first matched file in the dictionary\n",
    "            if matched_files:\n",
    "                print(f\"Matched files: {matched_files}\")\n",
    "                key = f\"{keyword}_{label_type}_{subset}\"\n",
    "                file_paths[key] = matched_files[0]\n",
    "            else:\n",
    "                # If no files are matched, log a warning\n",
    "                logging.warning(f\"No files matched for pattern: {pattern}\")\n",
    "    \n",
    "    return file_paths\n",
    "\n",
    "# If reading after division, build the file paths for the divided dataset\n",
    "if read_after_division:\n",
    "    # Call the `build_label_paths` function to retrieve paths based on the `date_path` and `data_set`\n",
    "    label_paths = build_label_paths(date_path, keyword, labels, data_set)\n",
    "\n",
    "    # Output the label paths in a unified format for the user\n",
    "    for key, path in label_paths.items():\n",
    "        print(f\"{key}: {path}\")\n",
    "else:\n",
    "    # If not reading after division, directly map the paths for the labels in the base load path\n",
    "    paths = {label: os.path.join(load_path, label) for label in labels}\n",
    "\n",
    "    # Output the paths in a unified format for the user\n",
    "    for label, path in paths.items():\n",
    "        print(f\"{label}: {path}\")\n",
    "\n",
    "# Function to ensure that a directory exists; creates it if it doesn't\n",
    "def ensure_directory_exists(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        # If the directory does not exist, create it\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created.\")\n",
    "    else:\n",
    "        # If the directory already exists, notify the user that no creation is necessary\n",
    "        print(f\"Directory '{directory}' already exists, no need to create.\")\n",
    "\n",
    "# Ensure the directory for storing statistics exists\n",
    "ensure_directory_exists(statistics_path)\n",
    "\n",
    "# Set up logging for the output with the specified log file path and format\n",
    "logging.basicConfig(filename=output_log_path, level=logging.INFO, format='%(message)s')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258a6618",
   "metadata": {},
   "source": [
    "# Function settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2040e64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:41.590993Z",
     "start_time": "2025-01-21T01:56:41.570176Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def count_samples_in_cn_cr(data_dir, file_pattern=\"*.csv\", remove_excluded=False, excluded_indices=[]):\n",
    "\n",
    "    if read_after_division:\n",
    "        sample_counts = {}\n",
    "        df = pd.read_csv(data_dir)  # Read the CSV file directly (if after division)\n",
    "        print(data_dir)\n",
    "        logging.info(data_dir)\n",
    "        sample_counts[basename(data_dir)] = len(df) + 1  # Account for potential header or extra row\n",
    "        print(sample_counts)\n",
    "        logging.info(sample_counts)\n",
    "    else:\n",
    "        file_list = glob.glob(join(data_dir, file_pattern))  # Get the list of files matching the pattern\n",
    "        sample_counts = {}\n",
    "        for file_path in file_list:\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)  # Read the CSV file\n",
    "                if remove_excluded:\n",
    "                    df = df[~df['index'].isin(excluded_indices)]  # Exclude specific indices if needed\n",
    "                sample_counts[basename(file_path)] = len(df)  # Count the number of rows in the file\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "        return sample_counts\n",
    "\n",
    "# Extract the method name from a file path by parsing the filename\n",
    "def extract_method_name(file_path):\n",
    "    \"\"\"Extract the method name from the file path by splitting the filename.\"\"\"\n",
    "    file_name = splitext(basename(file_path))[0]\n",
    "    method_name = file_name.split('_')[1]  # Method is the second part of the file name\n",
    "    return method_name\n",
    "\n",
    "\n",
    "# Function to process a list of data arrays to ensure they all have the same length,\n",
    "# padding shorter arrays or truncating longer arrays as needed.\n",
    "def process_data(data_list, max_length=None):\n",
    "\n",
    "    if max_length is None:\n",
    "        max_length = max(len(data) for data in data_list)  # Use the longest array length by default\n",
    "    \n",
    "    processed_data = []\n",
    "    for data in data_list:\n",
    "        if len(data) < max_length:  # If data is shorter than the target length\n",
    "            interp_func = interp1d(np.arange(len(data)), data, kind='linear', fill_value='extrapolate')\n",
    "            processed_data.append(interp_func(np.linspace(0, len(data)-1, max_length)))  # Interpolate to match max length\n",
    "        else:\n",
    "            processed_data.append(data[:max_length])  # Truncate if necessary\n",
    "    \n",
    "    return np.array(processed_data)\n",
    "\n",
    "\n",
    "# Function to generate statistical summaries and plots from CSV data files.\n",
    "def generate_statistics_and_plots(data_dir, plot_dir, precision=None, file_pattern=\"*.csv\", excluded_indices_file=None, remove_excluded=False, read_after_division=False):\n",
    "    if read_after_division:\n",
    "        file_list = [data_dir]  # If reading after division, use the single file\n",
    "        print(f\"Reading single file: {file_list}\")\n",
    "    else:\n",
    "        file_list = sorted(glob.glob(join(data_dir, file_pattern)))  # Get list of files matching the pattern\n",
    "        if not file_list:\n",
    "            print(f\"No files matching pattern {file_pattern} found in directory {data_dir}.\")\n",
    "            return\n",
    "    \n",
    "    stats = []\n",
    "    \n",
    "    # Ensure that the plot directory exists, creating it if necessary\n",
    "    ensure_directory_exists(plot_dir)\n",
    "\n",
    "    for file_path in file_list:\n",
    "        try:\n",
    "            if read_after_division:\n",
    "                df = pd.read_csv(file_path, delim_whitespace=True, header=None)  # Read TXT file if after division\n",
    "            else:\n",
    "                df = pd.read_csv(file_path)  # Default CSV reading method\n",
    "            \n",
    "            if df.empty:\n",
    "                print(f\"Warning: {file_path} is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Extract method name from the file path\n",
    "            full_method_name = extract_method_name(file_path)\n",
    "\n",
    "            # Split the method name if necessary (in case there are parts separated by '-')\n",
    "            method_name_parts = full_method_name.split('-')\n",
    "            method_name = method_name_parts[1].strip() if len(method_name_parts) > 1 else full_method_name\n",
    "\n",
    "            # Remove excluded indices if requested\n",
    "            if remove_excluded and excluded_indices_file:\n",
    "                excluded_indices_df = pd.read_csv(excluded_indices_file, header=None)[\"index\"].tolist()\n",
    "                df = df[~df.index.isin(excluded_indices_df)]\n",
    "\n",
    "            # Statistical description of the data column\n",
    "            if read_after_division:\n",
    "                data_column = df[0]  # Use the first column if reading after division\n",
    "            else:\n",
    "                data_column = df.iloc[:, 1]  # Otherwise, use the second column as data\n",
    "\n",
    "            description = data_column.describe()  # Get the descriptive statistics of the data\n",
    "            stats.append(description)\n",
    "\n",
    "            # Plotting\n",
    "\n",
    "            # Density plot\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.kdeplot(data_column, fill=True)\n",
    "            plt.xlabel(f\"{'Coordination Number' if 'cn' in data_dir else 'Bond length'} - {method_name}\", fontsize=16)\n",
    "            plt.ylabel(\"Density\", fontsize=16)\n",
    "            plt.title(\"Density Plot\", fontsize=16)\n",
    "            plt.xticks(rotation=0, fontsize=14)\n",
    "            plt.yticks(rotation=0, fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plot_filename = f\"{full_method_name}_density.png\"\n",
    "            plot_path = join(plot_dir, plot_filename)\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            print(f\"Plot saved: {plot_path}\")\n",
    "\n",
    "            # Box plot\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.boxplot(y=data_column, color='skyblue', medianprops={'color': 'red'})\n",
    "            plt.ylabel(f\"y ({method_name})\", fontsize=16)\n",
    "            plt.title(\"Box Plot\", fontsize=16)\n",
    "            x_label = \"Bond Length (Å)\" if \"cr\" in data_dir else \"Coordination Number\"\n",
    "            plt.xlabel(x_label, fontsize=16)\n",
    "            plt.xticks(rotation=0, fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plot_filename = f\"{full_method_name}_box.png\"\n",
    "            plot_path = join(plot_dir, plot_filename)\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            print(f\"Plot saved: {plot_path}\")\n",
    "\n",
    "            # Histogram\n",
    "            min_value = data_column.min()\n",
    "            max_value = data_column.max()\n",
    "            precision = precision or max(int(-np.floor(np.log10(data_column.abs().max()))), 0)\n",
    "            num_bins = min(30, int((max_value - min_value) / (10 ** -precision)))\n",
    "\n",
    "            bin_width = (max_value - min_value) / num_bins\n",
    "            bins = np.arange(min_value, max_value + bin_width, bin_width)\n",
    "\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            ax = sns.histplot(data_column, bins=bins, kde=False)\n",
    "            ax.set_xticks(bins + bin_width / 2)\n",
    "            ax.set_xticklabels([f'{bin_edge:.{precision}f}' for bin_edge in bins], rotation=45, ha='right')\n",
    "\n",
    "            num_patches = len(ax.patches)\n",
    "            print(f\"Number of patches: {num_patches}\")\n",
    "            if num_patches > 15:\n",
    "                plt.xticks(rotation=45, fontsize=12, fontstyle='italic')\n",
    "            else:\n",
    "                plt.xticks(rotation=0, fontsize=14)\n",
    "            plt.xlabel(f\"{'Coordination Number' if 'cn' in data_dir else 'Bond length'} - {method_name}\", fontsize=16)\n",
    "            plt.ylabel(\"Count\", fontsize=16)\n",
    "            plt.title(\"Histogram\", fontsize=16)\n",
    "            plt.tight_layout()\n",
    "\n",
    "            for p in ax.patches:\n",
    "                if p.get_height() > 0:\n",
    "                    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                                ha='center', va='center', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "            plot_filename = f\"{full_method_name}_hist.png\"\n",
    "            plot_path = join(plot_dir, plot_filename)\n",
    "            plt.savefig(plot_path)\n",
    "            plt.close()\n",
    "            print(f\"Plot saved: {plot_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    # Combine all statistical descriptions into a single DataFrame and save as CSV\n",
    "    stats_df = pd.concat(stats, axis=1)\n",
    "    stats_df.columns = [splitext(basename(f))[0] for f in file_list]\n",
    "    stats_df.to_csv(join(plot_dir, 'statistics_summary.csv'))\n",
    "    print(\"Statistics summary saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075f887",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed30389f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:41.624624Z",
     "start_time": "2025-01-21T01:56:41.592055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts for cr:\n",
      "cr_MinimumDistanceNN.csv: 5001 samples\n",
      "cr_JmolNN.csv: 5001 samples\n",
      "cr_VoronoiNN.csv: 5001 samples\n",
      "cr_CrystalNN.csv: 5001 samples\n",
      "cr_BrunnerNN_relative.csv: 5001 samples\n",
      "cr_EconNN.csv: 5001 samples\n",
      "Sample counts for cn:\n",
      "cn_CrystalNN.csv: 5001 samples\n",
      "cn_MinimumDistanceNN.csv: 5001 samples\n",
      "cn_JmolNN.csv: 5001 samples\n",
      "cn_BrunnerNN_relative.csv: 5001 samples\n",
      "cn_EconNN.csv: 5001 samples\n",
      "cn_VoronoiNN.csv: 5001 samples\n"
     ]
    }
   ],
   "source": [
    "for label in labels:\n",
    "    if read_after_division:\n",
    "        for subset in data_set:\n",
    "            keyword_label = f\"{keyword}_{label}_{subset}\" \n",
    "            print(keyword_label)\n",
    "            data_dir = paths[keyword_label]  \n",
    "            sample_counts = count_samples_in_cn_cr(data_dir, file_pattern=\"*.csv\", remove_excluded=None, excluded_indices=None)\n",
    "    else:\n",
    "        data_dir = paths[label]\n",
    "        sample_counts = count_samples_in_cn_cr(data_dir, file_pattern=\"*.csv\", remove_excluded=remove_excluded, excluded_indices=excluded_indices)\n",
    "        logging.info(f\"Sample counts for {label}:\")\n",
    "        print(f\"Sample counts for {label}:\")\n",
    "        for file_name, count in sample_counts.items():\n",
    "            print(f\"{file_name}: {count} samples\")\n",
    "            logging.info(f\"{file_name}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f667f01d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T01:56:47.625634Z",
     "start_time": "2025-01-21T01:56:41.625783Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing label: cr, data_dir: 0926-datasets/prepare/cr, output_dir: 0926-datasets/statistics_20250121_0956/cr_distribution_plots\n",
      "Directory '0926-datasets/statistics_20250121_0956/cr_distribution_plots' created.\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/BrunnerNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/BrunnerNN_box.png\n",
      "Number of patches: 24\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/BrunnerNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/CrystalNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/CrystalNN_box.png\n",
      "Number of patches: 9\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/CrystalNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/EconNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/EconNN_box.png\n",
      "Number of patches: 9\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/EconNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/JmolNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/JmolNN_box.png\n",
      "Number of patches: 8\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/JmolNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/MinimumDistanceNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/MinimumDistanceNN_box.png\n",
      "Number of patches: 10\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/MinimumDistanceNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/VoronoiNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/VoronoiNN_box.png\n",
      "Number of patches: 30\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cr_distribution_plots/VoronoiNN_hist.png\n",
      "Statistics summary saved.\n",
      "Processing label: cn, data_dir: 0926-datasets/prepare/cn, output_dir: 0926-datasets/statistics_20250121_0956/cn_distribution_plots\n",
      "Directory '0926-datasets/statistics_20250121_0956/cn_distribution_plots' created.\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/BrunnerNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/BrunnerNN_box.png\n",
      "Number of patches: 31\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/BrunnerNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/CrystalNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/CrystalNN_box.png\n",
      "Number of patches: 12\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/CrystalNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/EconNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/EconNN_box.png\n",
      "Number of patches: 11\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/EconNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/JmolNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/JmolNN_box.png\n",
      "Number of patches: 13\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/JmolNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/MinimumDistanceNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/MinimumDistanceNN_box.png\n",
      "Number of patches: 9\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/MinimumDistanceNN_hist.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/VoronoiNN_density.png\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/VoronoiNN_box.png\n",
      "Number of patches: 22\n",
      "Plot saved: 0926-datasets/statistics_20250121_0956/cn_distribution_plots/VoronoiNN_hist.png\n",
      "Statistics summary saved.\n"
     ]
    }
   ],
   "source": [
    "def label_statistics(labels, remove_excluded=False, cn_precision=None, cr_precision=None):\n",
    "    for label in labels:\n",
    "        if read_after_division:  # If the data is split after division\n",
    "            # Process data for each subset (e.g., train, valid, test)\n",
    "            for subset in data_set:\n",
    "                keyword_label = f\"{keyword}_{label}_{subset}\"\n",
    "                print(f\"Processing label: {keyword_label}\")\n",
    "                data_dir = paths[keyword_label]  # Get the directory for the current label subset\n",
    "                output_dir = join(statistics_path, f'{label}_{subset}_distribution_plots')  # Directory to save plots\n",
    "                \n",
    "                # Determine the appropriate precision based on the label type\n",
    "                precision = cn_precision if label == \"cn\" else cr_precision\n",
    "                \n",
    "                # Call the function to generate statistics and plots for the data\n",
    "                generate_statistics_and_plots(data_dir, output_dir, precision=precision, \n",
    "                                              remove_excluded=None, read_after_division=True)\n",
    "        else:  # If not reading after division, process the full data\n",
    "            data_dir = paths[label]  # Get the directory for the current label\n",
    "            output_dir = join(statistics_path, f'{label}_distribution_plots')  # Directory to save plots\n",
    "            print(f\"Processing label: {label}, data_dir: {data_dir}, output_dir: {output_dir}\")\n",
    "            \n",
    "            # Select precision based on the label type (coordination number or bond length)\n",
    "            precision = cn_precision if label == \"cn\" else cr_precision\n",
    "            generate_statistics_and_plots(data_dir, output_dir, precision=precision, \n",
    "                                          remove_excluded=remove_excluded)\n",
    "\n",
    "# Call the function based on whether the data is read after division\n",
    "if read_after_division:\n",
    "    label_statistics(labels, remove_excluded=None, cn_precision=cn_precision, cr_precision=cr_precision)\n",
    "else:\n",
    "    label_statistics(labels, remove_excluded=remove_excluded, cn_precision=cn_precision, cr_precision=cr_precision)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68433eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "223.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
