{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3e7d23f",
   "metadata": {},
   "source": [
    "# Program Description of Dataset Standardization (Module 9)\n",
    "\n",
    "This module focuses on preprocessing the divided and analyzed datasets. It performs standardization and, optionally, Principal Component Analysis (PCA) on the features of the training set, validation set, and test set. The main functions include:\n",
    "\n",
    "- **Standardization**: Scaling the features of the datasets to have zero mean and unit variance.\n",
    "- **PCA (Principal Component Analysis)**: An optional transformation that reduces the dimensionality of the data by transforming it into a set of orthogonal components. Whether PCA is applied is controlled by the `apply_pca` parameter.\n",
    "\n",
    "The module helps to ensure that the data used for modeling or analysis is well-conditioned and suitable for machine learning or statistical techniques.\n",
    "\n",
    "### Key Parameters:\n",
    "- **`apply_pca`**: This parameter manually sets whether the dataset should undergo PCA transformation. If set to `True`, PCA is applied; otherwise, standardization is performed without dimensionality reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e7076",
   "metadata": {},
   "source": [
    "contactsï¼šzhaohf@ihep.ac.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113aaea4",
   "metadata": {},
   "source": [
    "#  Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf107ecc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:02:01.766933Z",
     "start_time": "2025-01-21T02:02:01.331299Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b76398",
   "metadata": {},
   "source": [
    "# Parameter Settings\n",
    "\n",
    "## Input file path:\n",
    "- `'dir_data'` is the directory where the original datasets are located.\n",
    "- The files for features and labels for training, validation, and test datasets are loaded from this directory.\n",
    "  Example: `'0926-datasets-2/datasets(JmolNN)'`\n",
    "\n",
    "## Output file path:\n",
    "- `'dir_output'` is the directory where the processed datasets will be saved after applying the necessary transformations.\n",
    "  Example: `'0926-datasets/datasets(JmolNN)-pre-xmu-cn'`\n",
    "\n",
    "## PCA Conversion flag (`apply_pca`):\n",
    "- `'apply_pca'` is a flag indicating whether PCA (Principal Component Analysis) should be applied to the datasets.\n",
    "- Set to `True` to apply PCA for dimensionality reduction; `False` if no PCA transformation is needed.\n",
    "- Default is `False` (PCA will not be applied).\n",
    " be applied).\n",
    " be applied).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4419c0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:02:01.775226Z",
     "start_time": "2025-01-21T02:02:01.769397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0926-datasets/datasets(JmolNN)' exists.\n"
     ]
    }
   ],
   "source": [
    "# Set the input file path and output directory\n",
    "dir_data = '0926-datasets/datasets(JmolNN)'\n",
    "dir_output = os.path.join('0926-datasets', 'datasets(JmolNN)-pre-xmu-cn')\n",
    "# Check if the input directory exists\n",
    "if os.path.exists(dir_data):\n",
    "    print(f\"Directory '{dir_data}' exists.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Directory '{dir_data}' does not exist.\")\n",
    "\n",
    "# Create the output directory if it does not exist\n",
    "os.makedirs(dir_output, exist_ok=True)\n",
    "\n",
    "# Define a flag to determine whether to apply PCA \n",
    "apply_pca = False  # Set this to True if you want to apply PCA\n",
    "\n",
    "# Load the training, validation, and test datasets (features and labels)\n",
    "file_train_feature = os.path.join(dir_data, 'xmu_train_JmolNN.txt')\n",
    "file_train_label = os.path.join(dir_data, 'label_cn_train_JmolNN.txt')\n",
    "file_valid_feature = os.path.join(dir_data, 'xmu_valid_JmolNN.txt')\n",
    "file_valid_label = os.path.join(dir_data, 'label_cn_valid_JmolNN.txt')\n",
    "file_test_feature = os.path.join(dir_data, 'xmu_test_JmolNN.txt')\n",
    "file_test_label = os.path.join(dir_data, 'label_cn_test_JmolNN.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5516fc60",
   "metadata": {},
   "source": [
    " # Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "704cdf19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:02:05.760419Z",
     "start_time": "2025-01-21T02:02:01.776745Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler saved to: 0926-datasets/datasets(JmolNN)-pre-xmu-cn/scaler.pkl\n",
      "Standardized data has been saved to: 0926-datasets/datasets(JmolNN)-pre-xmu-cn\n",
      "Labels have been saved to: 0926-datasets/datasets(JmolNN)-pre-xmu-cn\n"
     ]
    }
   ],
   "source": [
    "# Function: Generate result file path\n",
    "def generate_file_path(input_file, output_dir, suffix='.txt', pca_applied=False):\n",
    "    filename = os.path.basename(input_file)  # Extract the input file name\n",
    "    # Append '_pca' suffix only if PCA is applied\n",
    "    result_filename = f'{filename.split(\".\")[0]}{\"_pca\" if pca_applied else \"\"}{suffix}'\n",
    "    return os.path.join(output_dir, result_filename)\n",
    "\n",
    "# Read input data\n",
    "X_train = np.loadtxt(file_train_feature)\n",
    "y_train = np.loadtxt(file_train_label, dtype=float)\n",
    "X_valid = np.loadtxt(file_valid_feature)\n",
    "y_valid = np.loadtxt(file_valid_label, dtype=float)\n",
    "X_test = np.loadtxt(file_test_feature)\n",
    "y_test = np.loadtxt(file_test_label, dtype=float)\n",
    "\n",
    "# Standardize the data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Save the StandardScaler model for future use\n",
    "scaler_path = os.path.join(dir_output, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"Scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Apply PCA if specified\n",
    "if apply_pca:\n",
    "    # Apply PCA and extract as many components as the number of features\n",
    "    pca = PCA(n_components=X_train.shape[1])\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_valid_pca = pca.transform(X_valid)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # Generate file paths for the PCA-transformed data\n",
    "    train_file = generate_file_path(file_train_feature, dir_output, pca_applied=True)\n",
    "    valid_file = generate_file_path(file_valid_feature, dir_output, pca_applied=True)\n",
    "    test_file = generate_file_path(file_test_feature, dir_output, pca_applied=True)\n",
    "\n",
    "    # Save PCA-transformed data to files\n",
    "    np.savetxt(train_file, X_train_pca)\n",
    "    np.savetxt(valid_file, X_valid_pca)\n",
    "    np.savetxt(test_file, X_test_pca)\n",
    "\n",
    "    print(f\"PCA-transformed data has been saved to: {dir_output}\")\n",
    "else:\n",
    "    # Save the standardized data without PCA\n",
    "    train_file = generate_file_path(file_train_feature, dir_output)\n",
    "    valid_file = generate_file_path(file_valid_feature, dir_output)\n",
    "    test_file = generate_file_path(file_test_feature, dir_output)\n",
    "\n",
    "    np.savetxt(train_file, X_train)\n",
    "    np.savetxt(valid_file, X_valid)\n",
    "    np.savetxt(test_file, X_test)\n",
    "\n",
    "    print(f\"Standardized data has been saved to: {dir_output}\")\n",
    "\n",
    "# Generate file paths for label files and save them\n",
    "train_label_file = generate_file_path(file_train_label, dir_output)\n",
    "valid_label_file = generate_file_path(file_valid_label, dir_output)\n",
    "test_label_file = generate_file_path(file_test_label, dir_output)\n",
    "\n",
    "np.savetxt(train_label_file, y_train)\n",
    "np.savetxt(valid_label_file, y_valid)\n",
    "np.savetxt(test_label_file, y_test)\n",
    "\n",
    "print(f\"Labels have been saved to: {dir_output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ad6c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9adedc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
