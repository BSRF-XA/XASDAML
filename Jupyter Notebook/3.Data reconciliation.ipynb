{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46c2959-1056-44ec-8fe1-bceb33140414",
   "metadata": {},
   "source": [
    "# Program Description: Data Reconciliation (Module 3)\n",
    "## Overview:\n",
    "This module performs data reconciliation for the results obtained from **Module 1** (XAS Simulation). The purpose is to apply interpolation and data preprocessing to the features `['chi', 'xmu', 'rdf']` obtained from these modules. The module ensures that the features are aligned, resampled, and interpolated as necessary for consistent comparison and further analysis.\n",
    "\n",
    "## Input Files:\n",
    "- **Input Features (before interpolation)**: \n",
    "  - **Features**: `['chi', 'xmu', 'rdf']`\n",
    "  - **Description**: These are the raw features (chi, xmu, rdf) obtained from **Module 1** and **Module 2**.\n",
    "  - **Format**: CSV or data files containing the raw feature data.\n",
    "\n",
    "## Output Files:\n",
    "- **Output Features (after interpolation)**: \n",
    "  - **Features**: `['chi', 'xmu', 'rdf']`\n",
    "  - **Description**: These are the features that have undergone interpolation and preprocessing.\n",
    "  - **Format**: CSV or data files containing the interpolated feature data.\n",
    "  \n",
    "The features are saved with new interpolated values that align across the samples, making the data consistent and ready for further analysis or visualization.\n",
    "\n",
    "## Process:\n",
    "1. **Interpolation**: \n",
    "   - The module applies interpolation techniques to the features `['chi', 'xmu', 'rdf']` to ensure that data points across the samples are consistently aligned. This allows for comparisons between features that were calculated with different resolutions or sample frequencies.\n",
    "   - Interpolation methods such as linear interpolation, cubic spline, or other relevant techniques are used depending on the data requirements.\n",
    "\n",
    "2. **Data Alignment**: \n",
    "   - Ensures that the features from both modules are aligned in terms of sample points (k-values, energy, or other relevant metrics), and missing or unaligned data is handled through the interpolation process.\n",
    "\n",
    "3. **Output Data**: \n",
    "   - After the interpolation is applied, the processed data is saved to new files with updated feature values, allowing users to continue their analysis with the reconciled and consistent data.\n",
    "\n",
    "## Example Workflow:\n",
    "1. **Input**: Raw `['chi', 'xmu', 'rdf']` from **Module 1** (XAS Simulation) and **Module 2** (Structure Descriptors).\n",
    "2. **Process**: Interpolate data, reconcile features, and ensure consistency across the samples.\n",
    "3. **Output**: Interpolated `['chi', 'xmu', 'rdf']`, saved in a specified directory.\n",
    "\n",
    "## Notes:\n",
    "- This module is especially useful when the data from **Module 1**  are not directly comparable due to varying resolutions or inconsistencies in sampling points. The interpolation process reconciles the differences to ensure uniformity across the dataset.\n",
    "- The interpolated data is stored in the same file format as the input data, making it easy to access and further process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b58fde-a237-4b87-a3f0-516006d5cc8e",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0471e-1d3c-4266-858f-3a7c3f7bb603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, splitext, split,basename\n",
    "import sys \n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from larch import Group\n",
    "from larch.xafs import pre_edge\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a016290-5bbe-4500-9d2c-0f726f87fac0",
   "metadata": {},
   "source": [
    "# Version Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b899884-374c-4f6f-843d-c80bcb7a277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_python_version():\n",
    "    return sys.version\n",
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        version = getattr(module, '__version__', None)\n",
    "        if version:\n",
    "            return version\n",
    "        else:\n",
    "            return pkg_resources.get_distribution(package_name).version\n",
    "    except (ImportError, AttributeError, pkg_resources.DistributionNotFound):\n",
    "        return \"Version info not found\"\n",
    "\n",
    "packages = ['pandas', 'numpy', 'sklearn', 'larch']\n",
    "for package in packages:\n",
    "    print(f\"{package}: {get_package_version(package)}\")\n",
    "print(f\"Python: {get_python_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60112138-6a33-4f9f-b1fc-98b68853a13a",
   "metadata": {},
   "source": [
    "# Parameter Settings\n",
    "\n",
    "## Input Files:\n",
    "- **`load_path`**: \n",
    "  - **Description**: Specifies the directory path where the dataset files are stored.\n",
    "  - **Usage**: Ensure this path points to the folder containing the data to be used for further processing.\n",
    "  - **Example**: `\"/media/dell-hd/data1/datasets/Au-datasets\"`\n",
    "\n",
    "## Data Types:\n",
    "- **`features`**: \n",
    "  - **Description**: A list of features to be used for processing.\n",
    "  - **Usage**: Defines the types of data (such as chi, xmu, rdf) that will be analyzed.\n",
    "  - **Example**: `['chi', 'xmu', 'rdf']`\n",
    "\n",
    "## Label Types:\n",
    "- **`labels`**: \n",
    "  - **Description**: Defines the types of labels to be used in the analysis. \n",
    "  - **Usage**: Customize based on the specific labels you want to  analyze.\n",
    "\n",
    "## Directory Checks:\n",
    "- **`load_path` existence check**: \n",
    "  - **Description**: Verifies that the specified `load_path` exists before proceeding with further operations.\n",
    "  - **Usage**: Raises an error if the path does not exist, ensuring that the user has provided a valid directory.\n",
    "\n",
    "## Output Paths:\n",
    "- **`save_data_path`**: \n",
    "  - **Description**: Directory path where the final processed data will be saved.\n",
    "  - **Usage**: This path stores the datasets generated by the method and analysis.\n",
    "  - **Example**: `os.path.join(load_path, f\"datasets({method})\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e4242-3ef0-432e-9c79-ef7099a25ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the method and load path for the dataset\n",
    "load_path = \"/media/dell-hd/data1/datasets/Au-datasets\"\n",
    "\n",
    "# Define the feature types to be used in the analysis (e.g., chi, xmu, rdf)\n",
    "features = ['chi', 'xmu', 'rdf']\n",
    "\n",
    "# Check if the specified load path exists\n",
    "if os.path.exists(load_path):\n",
    "    print(f\"File '{load_path}' exists.\")  # Print a message if the path exists\n",
    "else:\n",
    "    raise FileNotFoundError(f\"File '{load_path}' does not exist.\")  # Raise an error if the path does not exist\n",
    "\n",
    "# Create a save path for the processed data, appending the method name to the dataset path\n",
    "save_data_path = os.path.join(load_path, f\"datasets({method})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23588c28-4f24-40d2-9b9a-c9cafedcabf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read chi data from CSV files, optionally save and plot the data\n",
    "def get_chi(path, n=-1, save_path=False, plot=False):\n",
    "    plot_dat = None\n",
    "    chi = []\n",
    "    # Sort CSV files by filename (assuming filenames are numerical) and limit to 'n' files\n",
    "    file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "    \n",
    "    for file in file_list:\n",
    "        index = int(splitext(basename(file))[0])\n",
    "        dat = pd.read_csv(file)\n",
    "        \n",
    "        # Save the data to the specified path if 'save_path' is set to True\n",
    "        if save_path:\n",
    "            dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "        \n",
    "        # If the file index matches the 'plot' value, store the data for plotting\n",
    "        if index == plot:\n",
    "            plot_dat = dat\n",
    "        \n",
    "        # Remove the first row if it corresponds to a chi value of 0\n",
    "        if dat.k[0] == 0:\n",
    "            dat = dat[1:]\n",
    "        \n",
    "        chi.append((index, dat.chi.values))\n",
    "    \n",
    "    # Sort chi data by index and extract the chi values\n",
    "    chi.sort(key=lambda x: x[0])\n",
    "    chi = [x[1] for x in chi]\n",
    "    \n",
    "    # If 'plot' is specified, plot the chi data for the chosen sample\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(1, 1)\n",
    "        ax.plot(plot_dat.k, plot_dat.chi * (plot_dat.k ** 2))  # Chi*k^2 plot\n",
    "        plt.show()\n",
    "    \n",
    "    return np.array(chi)\n",
    "\n",
    "# Function to read xmu data, perform pre-edge correction, and return the processed data\n",
    "def get_xmu(path, n=-1, save_path=False, plot=False):\n",
    "    plot_dat = None\n",
    "    plot_dat_new = None\n",
    "    emin, emax = float('inf'), float('-inf')\n",
    "    \n",
    "    # Sort CSV files and limit to 'n' files\n",
    "    file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "    \n",
    "    for file in file_list:\n",
    "        index = int(splitext(basename(file))[0])\n",
    "        dat = pd.read_csv(file)\n",
    "        \n",
    "        # Store the data for plotting if 'plot' matches the current index\n",
    "        if index == plot:\n",
    "            plot_dat = dat\n",
    "        \n",
    "        dat = Group(energy=dat.energy, mu=dat.mu)\n",
    "        pre_edge(dat)  # Perform pre-edge correction\n",
    "        emin = min(emin, dat.e0)\n",
    "        emax = max(emax, dat.e0)\n",
    "    \n",
    "    # Interpolate the data to a uniform energy grid\n",
    "    xmu = []\n",
    "    xvals = np.linspace(emin - 10, emax + 200, 1000)\n",
    "    \n",
    "    for file in file_list:\n",
    "        index = int(splitext(basename(file))[0])\n",
    "        dat = pd.read_csv(file)\n",
    "        yvals = np.interp(xvals, dat.energy, dat.mu)\n",
    "        \n",
    "        # Store the interpolated data for plotting\n",
    "        if index == plot:\n",
    "            plot_dat_new = (xvals, yvals)\n",
    "        \n",
    "        xmu.append((index, yvals))\n",
    "    \n",
    "    # Sort xmu data by index and extract the y-values\n",
    "    xmu.sort(key=lambda x: x[0])\n",
    "    xmu = [x[1] for x in xmu]\n",
    "    \n",
    "    # Plot the original and interpolated xmu data if 'plot' is set\n",
    "    if plot and plot_dat_new is not None:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(plot_dat.energy, plot_dat.mu)  # Original data\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(plot_dat_new[0], plot_dat_new[1])  # Interpolated data\n",
    "        plt.show()\n",
    "    \n",
    "    # Save the processed xmu data to CSV files if 'save_path' is specified\n",
    "    if save_path:\n",
    "        for i, file in enumerate(file_list):\n",
    "            dat = pd.DataFrame({\"energy\": xvals, \"mu\": xmu[i]})\n",
    "            dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "    \n",
    "    return xmu\n",
    "\n",
    "# Function to read norm spectrum data, perform pre-edge correction, and return the processed data\n",
    "def get_norm(path, n=-1, save_path=False, plot=False):\n",
    "    plot_dat = None\n",
    "    plot_dat_new = None\n",
    "    emin, emax = float('inf'), float('-inf')\n",
    "    \n",
    "    # Sort CSV files and limit to 'n' files\n",
    "    file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "    \n",
    "    for file in file_list:\n",
    "        index = int(splitext(basename(file))[0])\n",
    "        dat = pd.read_csv(file)\n",
    "        \n",
    "        # Store the data for plotting if 'plot' matches the current index\n",
    "        if index == plot:\n",
    "            plot_dat = dat\n",
    "        \n",
    "        dat = Group(energy=dat.energy, mu=dat.norm)\n",
    "        pre_edge(dat)  # Perform pre-edge correction\n",
    "        emin = min(emin, dat.e0)\n",
    "        emax = max(emax, dat.e0)\n",
    "    \n",
    "    # Interpolate the data to a uniform energy grid\n",
    "    norm = []\n",
    "    xvals = np.linspace(emin - 30, emax + 400, 500)\n",
    "    \n",
    "    for file in file_list:\n",
    "        index = int(splitext(basename(file))[0])\n",
    "        dat = pd.read_csv(file)\n",
    "        yvals = np.interp(xvals, dat.energy, dat.norm)\n",
    "        \n",
    "        # Store the interpolated data for plotting\n",
    "        if index == plot:\n",
    "            plot_dat_new = (xvals, yvals)\n",
    "        \n",
    "        norm.append((index, yvals))\n",
    "    \n",
    "    # Sort norm data by index and extract the y-values\n",
    "    norm.sort(key=lambda x: x[0])\n",
    "    norm = [x[1] for x in norm]\n",
    "    \n",
    "    # Plot the original and interpolated norm data if 'plot' is set\n",
    "    if plot and plot_dat_new is not None:\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(plot_dat.energy, plot_dat.norm)  # Original data\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(plot_dat_new[0], plot_dat_new[1])  # Interpolated data\n",
    "        plt.show()\n",
    "    \n",
    "    # Save the processed norm data to CSV files if 'save_path' is specified\n",
    "    if save_path:\n",
    "        for i, file in enumerate(file_list):\n",
    "            dat = pd.DataFrame({\"energy\": xvals, \"norm\": norm[i]})\n",
    "            dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "    \n",
    "    return norm\n",
    "\n",
    "# Function to read RDF (Radial Distribution Function) data and return the processed data\n",
    "def get_rdf(path, n=-1, save_path=False, plot=False):\n",
    "    plot_dat = None\n",
    "    rdf = []\n",
    "    \n",
    "    # Sort CSV files and limit to 'n' files\n",
    "    file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "    \n",
    "    for file in file_list:\n",
    "        index = int(splitext(basename(file))[0])\n",
    "        dat = pd.read_csv(file)\n",
    "        \n",
    "        # Save the data to the specified path if 'save_path' is set\n",
    "        if save_path:\n",
    "            dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "        \n",
    "        # Store the data for plotting if 'plot' matches the current index\n",
    "        if index == plot:\n",
    "            plot_dat = dat\n",
    "        \n",
    "        rdf.append((index, dat['g(r)'].values))\n",
    "    \n",
    "    # Sort RDF data by index and extract the g(r) values\n",
    "    rdf.sort(key=lambda x: x[0])\n",
    "    rdf = [x[1] for x in rdf]\n",
    "    \n",
    "    # Plot the RDF data if 'plot' is set\n",
    "    if plot and plot_dat is not None:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(plot_dat['r'], plot_dat['g(r)'])  # RDF plot\n",
    "        plt.xlabel(\"r\")\n",
    "        plt.ylabel(\"g(r)\")\n",
    "        plt.title(\"RDF Plot\")\n",
    "        plt.show()\n",
    "    \n",
    "    return np.array(rdf)\n",
    "\n",
    "# Function to read wavelet transform (WT) data and return the processed wavelet data\n",
    "def get_wt(path, rate=1200, n=40, save_path=False, plot=False):\n",
    "    if save_path:\n",
    "        save_path1, save_path2 = join(save_path, \"1\"), join(save_path, \"2\")\n",
    "        if not os.path.exists(save_path1):\n",
    "            os.makedirs(save_path1)\n",
    "        if not os.path.exists(save_path2):\n",
    "            os.makedirs(save_path2)\n",
    "    \n",
    "    # Sort CSV files and limit to 'n' files\n",
    "    file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "    dat = pd.read_csv(file_list[0])\n",
    "    \n",
    "    k_list, r_list = sorted(list(set(dat.k.values))), sorted(list(set(dat.r.values)))\n",
    "    if 0 in k_list:\n",
    "        k_list.remove(0.0)\n",
    "    k_list, r_list = k_list[\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718e13eb-466d-423b-8ea3-172afb01e12e",
   "metadata": {},
   "source": [
    "#   Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd8dd54-ddbb-4477-b40b-1f9a3299bfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_data(features, n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
