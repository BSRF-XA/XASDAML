{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bde2445",
   "metadata": {},
   "source": [
    "# Program Description: Dataset Division (Module 8)\n",
    "\n",
    "## Overview:\n",
    "This module is responsible for preprocessing the generated CSV dataset, dividing the dataset into three subsets: **Train**, **Validation**, and **Test**. The division follows a common ratio of **7:2:1** (train:valid:test). The resulting subsets are saved in `.txt` format and stored in the `datasets` folder under the current working path. Additionally, a log file (`out_log.txt`) is created in the `prepare` folder, which records the size of each divided dataset.\n",
    "\n",
    "## Key Steps:\n",
    " \n",
    "1. **Dataset Division**:\n",
    "   - The filtered dataset is randomly split into **Train**, **Validation**, and **Test** sets in a 7:2:1 ratio.\n",
    "   \n",
    "2. **Save Results**:\n",
    "   - Each dataset subset (Train, Valid, Test) is saved in a `.txt` format inside the `datasets` folder, making it ready for further analysis or modeling.\n",
    "   \n",
    "3. **Logging**:\n",
    "   - A log file (`out_log.txt`) is saved in the `prepare` folder, which includes details about the dataset sizes (number of samples in each subset) and any additional relevant information.\n",
    "\n",
    "## Output:\n",
    "- Three `.txt` files stored in the `datasets` folder:\n",
    "  - **train.txt**: Contains the training samples.\n",
    "  - **valid.txt**: Contains the validation samples.\n",
    "  - **test.txt**: Contains the testing samples.\n",
    "  \n",
    "- **out_log.txt**: A log file that provides the size of each dataset (Train, Valid, Test) and any preprocessing information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ad5923",
   "metadata": {},
   "source": [
    "contacts：zhaohf@ihep.ac.cn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9843d0c3",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72f85b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.867018Z",
     "start_time": "2025-01-21T02:00:09.762263Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join, splitext, split,basename\n",
    "import sys \n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from larch import Group\n",
    "from larch.xafs import pre_edge\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d513c06",
   "metadata": {},
   "source": [
    "## Version Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "946cab2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.874792Z",
     "start_time": "2025-01-21T02:00:10.869330Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 2.0.3\n",
      "numpy: 1.23.5\n",
      "sklearn: 1.3.2\n",
      "larch: 0.9.76\n",
      "Python: 3.8.15 | packaged by conda-forge | (default, Nov 22 2022, 08:46:39) \n",
      "[GCC 10.4.0]\n"
     ]
    }
   ],
   "source": [
    "def get_python_version():\n",
    "    return sys.version\n",
    "def get_package_version(package_name):\n",
    "    try:\n",
    "        module = __import__(package_name)\n",
    "        version = getattr(module, '__version__', None)\n",
    "        if version:\n",
    "            return version\n",
    "        else:\n",
    "            return pkg_resources.get_distribution(package_name).version\n",
    "    except (ImportError, AttributeError, pkg_resources.DistributionNotFound):\n",
    "        return \"Version info not found\"\n",
    "\n",
    "packages = ['pandas', 'numpy', 'sklearn', 'larch']\n",
    "for package in packages:\n",
    "    print(f\"{package}: {get_package_version(package)}\")\n",
    "print(f\"Python: {get_python_version()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000d23ca",
   "metadata": {},
   "source": [
    "# Parameter Settings\n",
    "- `method`: Specifies the method used to calculate the label in the data to be processed.\n",
    "- `load_path`: The input file or directory where the dataset is located.\n",
    "- `features`: A list of feature data variables used for dividing the dataset.\n",
    "- `label_types`: The label data selection from the input file.\n",
    "- All processed data will be saved in the `datasets` file within the current foder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c17c6cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.885166Z",
     "start_time": "2025-01-21T02:00:10.875913Z"
    },
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '0926-datasets/prepare' exists.\n"
     ]
    }
   ],
   "source": [
    "# Define the method and the data load path\n",
    "method = \"JmolNN\"\n",
    "dateset_path= \"0926-datasets\"\n",
    "load_path = os.path.join(dateset_path, \"prepare\")\n",
    "# Define the data types (features) to be used\n",
    "features = ['chi', 'xmu', 'rdf']\n",
    "# Define the label types\n",
    "label_types = [\"cn\", \"cr\"]\n",
    "# Check if the load path exists\n",
    "if os.path.exists(load_path):\n",
    "    print(f\"Directory '{load_path}' exists.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Directory '{load_path}' does not exist.\")\n",
    "# Create paths for preparing and saving data\n",
    "#prepare_path = os.path.join(load_path, f\"prepare({method})\")\n",
    "save_data_path = os.path.join(dateset_path, f\"datasets({method})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4b4fea",
   "metadata": {},
   "source": [
    "- `n`: The number of samples to be processed.\n",
    "- `train_set`, `valid_set`, `test_set`: The ratios for dividing the dataset into training, validation, and test sets, respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40e4a27a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.889607Z",
     "start_time": "2025-01-21T02:00:10.886910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the number of samples to preprocess\n",
    "n = 5001\n",
    "# Define the dataset split ratios for train, validation, and test sets\n",
    "train_set = 0.7\n",
    "valid_set = 0.2\n",
    "test_set = 0.1\n",
    "# Define the file suffix (extension)\n",
    "suffix = \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ec08ab7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.906073Z",
     "start_time": "2025-01-21T02:00:10.891093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path information has been stored in out_log.txt.\n",
      "Total number of samples: 5001\n"
     ]
    }
   ],
   "source": [
    "# Generate paths for each data type and store in variables\n",
    "for data_type in features:\n",
    "    globals()[f\"{data_type}_path\"] = join(load_path, data_type)\n",
    "\n",
    "# Construct label paths for the specified method\n",
    "for label_type in label_types:\n",
    "    globals()[f\"{label_type}_label_path\"] = join(load_path, f\"{label_type}/{label_type}_{method}.csv\")\n",
    "\n",
    "excluded_indices_file = join(dateset_path, f\"indices_to_move_{method}.csv\")\n",
    "\n",
    "# Create a dictionary to store paths, showing only common directory paths\n",
    "for data_type in features:\n",
    "    globals()[f\"save_{data_type}_path\"] = join(load_path, data_type)\n",
    "for label_type in label_types:\n",
    "    globals()[f\"save_label_{label_type}\"] = join(load_path, f\"label_{label_type}.csv\")\n",
    "\n",
    "# Check and create necessary directories\n",
    "paths_to_check = [load_path, save_data_path] + [globals()[f'save_{data_type}_path'] for data_type in features]\n",
    "for path in paths_to_check:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "# Create CSV files for labels if they do not exist\n",
    "for label_type in label_types:\n",
    "    label_csv_path = globals()[f'save_label_{label_type}']\n",
    "    if not os.path.exists(label_csv_path):\n",
    "        with open(label_csv_path, 'w'):\n",
    "            pass  # Create an empty CSV file\n",
    "\n",
    "# Store path information into a file\n",
    "with open(join(save_data_path, \"out_log.txt\"), \"w\") as f:\n",
    "    for label_type in label_types:\n",
    "        f.write(f\"Label Path ({label_type.upper()}): {globals()[f'{label_type}_label_path']}\\n\")\n",
    "    f.write(f\"Excluded Indices File Path: {excluded_indices_file}\\n\")\n",
    "    f.write(f\"Data Save Path: {save_data_path}\\n\\n\")\n",
    "\n",
    "    for data_type in features:\n",
    "        f.write(f\"{data_type} Data Path: {globals()[f'{data_type}_path']}\\n\")\n",
    "        f.write(f\"Save Path: {globals()[f'save_{data_type}_path']}\\n\")\n",
    "\n",
    "    for label_type in label_types:\n",
    "        f.write(f\"Save Path (label_{label_type}): {globals()[f'save_label_{label_type}']}\\n\")\n",
    "print(\"Path information has been stored in out_log.txt.\")\n",
    "\n",
    "# List files in the xmu directory and count the number of samples\n",
    "xmu_files = os.listdir(xmu_path)\n",
    "num_samples = len(xmu_files)\n",
    "print(f\"Total number of samples: {num_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea5cdf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.913729Z",
     "start_time": "2025-01-21T02:00:10.907624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check and create necessary directories if they do not exist\n",
    "#for path in [prepare_path, save_data_path] + [globals()[f\"save_{data_type}_path\"] for data_type in features]:\n",
    "    #if not os.path.exists(path):\n",
    "        #os.makedirs(path)\n",
    "#excluded_indices_file = join(dateset_path, f\"indices_to_move_{method}.csv\")\n",
    "# Read excluded indices if it exists\n",
    "if os.path.exists(excluded_indices_file):\n",
    "    excluded_indices = pd.read_csv(excluded_indices_file)[\"index\"].tolist()\n",
    "else:\n",
    "    excluded_indices = []  # Initialize an empty list if the file does not exist\n",
    "\n",
    "# Create empty label files if they do not exist\n",
    "for label_type in label_types:\n",
    "    save_label_file = globals()[f\"save_label_{label_type}\"]\n",
    "    if not os.path.exists(save_label_file):\n",
    "        pd.DataFrame(columns=[\"index\", \"value\"]).to_csv(save_label_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b796dd46",
   "metadata": {},
   "source": [
    "# Function settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b76f9532",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.934423Z",
     "start_time": "2025-01-21T02:00:10.915166Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the function to read the data, read the chi spectrum data\n",
    "# def get_chi(path, n=-1, save_path=False, plot=False):\n",
    "#     plot_dat = None\n",
    "#     chi = []\n",
    "#     file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "#     for file in file_list:\n",
    "#         index = int(splitext(basename(file))[0])\n",
    "#         dat = pd.read_csv(file)\n",
    "#         if save_path:\n",
    "#             dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "#         if index == plot:\n",
    "#             plot_dat = dat\n",
    "#         if dat.k[0] == 0:\n",
    "#             dat = dat[1:]\n",
    "#         chi.append((index, dat.chi.values))\n",
    "#     chi.sort(key=lambda x: x[0])\n",
    "#     chi = [x[1] for x in chi]\n",
    "#     if plot:\n",
    "#         fig, ax = plt.subplots(1, 1)\n",
    "#         ax.plot(plot_dat.k, plot_dat.chi * (plot_dat.k ** 2))\n",
    "#         plt.show()\n",
    "#     return np.array(chi)\n",
    "# # 定义读取数据的函数，读取xmu谱数据，并进行差分\n",
    "# def get_xmu(path, n=-1, save_path=False, plot=False):\n",
    "#     plot_dat = None\n",
    "#     plot_dat_new = None\n",
    "#     file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "#     emin, emax = float('inf'), float('-inf')\n",
    "#     for file in file_list:\n",
    "#         index = int(splitext(basename(file))[0])\n",
    "#         dat = pd.read_csv(file)\n",
    "#         if index == plot:\n",
    "#             plot_dat = dat\n",
    "#         dat = Group(energy=dat.energy, mu=dat.mu)\n",
    "#         pre_edge(dat)\n",
    "#         emin = min(emin, dat.e0)\n",
    "#         emax = max(emax, dat.e0)\n",
    "    \n",
    "#     xmu = []\n",
    "#     xvals = np.linspace(emin-10, emax+200, 1000)\n",
    "#     for file in file_list:\n",
    "#         index = int(splitext(basename(file))[0])\n",
    "#         dat = pd.read_csv(file)\n",
    "#         yvals = np.interp(xvals, dat.energy, dat.mu)\n",
    "#         if index == plot:\n",
    "#             plot_dat_new = (xvals, yvals)\n",
    "#         xmu.append((index, yvals))\n",
    "#     xmu.sort(key=lambda x: x[0])\n",
    "#     xmu = [x[1] for x in xmu]\n",
    "#     if plot and plot_dat_new is not None:\n",
    "#         plt.figure(figsize=(14, 6))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.plot(plot_dat.energy, plot_dat.mu)\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.plot(plot_dat_new[0], plot_dat_new[1])\n",
    "#         plt.show()\n",
    "#     if save_path:\n",
    "#         for i, file in enumerate(file_list):\n",
    "#             dat = pd.DataFrame({\"energy\": xvals, \"mu\": xmu[i]})\n",
    "#             dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "#     return xmu\n",
    "# # Define the function to read data and read norm spectrum data\n",
    "# def get_norm(path, n=-1, save_path=False, plot=False):\n",
    "#     plot_dat = None\n",
    "#     plot_dat_new = None\n",
    "#     file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "#     emin, emax = float('inf'), float('-inf')\n",
    "#     for file in file_list:\n",
    "#         index = int(splitext(basename(file))[0])\n",
    "#         dat = pd.read_csv(file)\n",
    "#         if index == plot:\n",
    "#             plot_dat = dat\n",
    "#         dat = Group(energy=dat.energy, mu=dat.norm)\n",
    "#         pre_edge(dat)\n",
    "#         emin = min(emin, dat.e0)\n",
    "#         emax = max(emax, dat.e0)\n",
    "    \n",
    "#     norm = []\n",
    "#     xvals = np.linspace(emin-30, emax+400, 500)\n",
    "#     for file in file_list:\n",
    "#         index = int(splitext(basename(file))[0])\n",
    "#         dat = pd.read_csv(file)\n",
    "#         yvals = np.interp(xvals, dat.energy, dat.norm)\n",
    "#         if index == plot:\n",
    "#             plot_dat_new = (xvals, yvals)\n",
    "#         norm.append((index, yvals))\n",
    "#     norm.sort(key=lambda x: x[0])\n",
    "#     norm = [x[1] for x in norm]\n",
    "#     if plot and plot_dat_new is not None:\n",
    "#         plt.figure(figsize=(14, 6))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         plt.plot(plot_dat.energy, plot_dat.norm)\n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         plt.plot(plot_dat_new[0], plot_dat_new[1])\n",
    "#         plt.show()\n",
    "#     if save_path:\n",
    "#         for i, file in enumerate(file_list):\n",
    "#             dat = pd.DataFrame({\"energy\": xvals, \"norm\": norm[i]})\n",
    "#             dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "#     return norm\n",
    "# # Define the function to read data and read rdf data\n",
    "# def get_rdf(path, n=-1, save_path=False, plot=False):\n",
    "#     plot_dat = None\n",
    "#     rdf = []\n",
    "#     file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "#     for file in file_list:\n",
    "#         index = int(splitext(basename(file))[0])\n",
    "#         dat = pd.read_csv(file)\n",
    "#         if save_path:\n",
    "#             dat.to_csv(join(save_path, basename(file)), index=False)\n",
    "#         if index == plot:\n",
    "#             plot_dat = dat\n",
    "#         rdf.append((index, dat['g(r)'].values))\n",
    "#     rdf.sort(key=lambda x: x[0])\n",
    "#     rdf = [x[1] for x in rdf]\n",
    "#     if plot and plot_dat is not None:\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(plot_dat['r'], plot_dat['g(r)'])\n",
    "#         plt.xlabel(\"r\")\n",
    "#         plt.ylabel(\"g(r)\")\n",
    "#         plt.title(\"RDF Plot\")\n",
    "#         plt.show()\n",
    "#     return np.array(rdf)\n",
    "# #Function to read wavelet change data\n",
    "# def get_wt(path, rate=1200, n=40, save_path=False, plot=False):\n",
    "#     if save_path:\n",
    "#         save_path1, save_path2 = join(save_path, \"1\"), join(save_path, \"2\")\n",
    "#         if not os.path.exists(save_path1):\n",
    "#             os.makedirs(save_path1)\n",
    "#         if not os.path.exists(save_path2):\n",
    "#             os.makedirs(save_path2)\n",
    "#     file_list = sorted(glob.glob(join(path, \"*.csv\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "#     dat = pd.read_csv(file_list[0])\n",
    "#     k_list, r_list = sorted(list(set(dat.k.values))), sorted(list(set(dat.r.values)))\n",
    "#     if 0 in k_list:\n",
    "#         k_list.remove(0.0)\n",
    "#     k_list, r_list = k_list[::math.ceil(len(k_list)/40)], r_list[::math.ceil(len(r_list)/40)]\n",
    "    \n",
    "#     wt = np.array([])\n",
    "#     all_dat = []\n",
    "#     for file in file_list:\n",
    "#         index = int(splitext(basename(file))[0])\n",
    "#         dat = pd.read_csv(file)\n",
    "#         dat = dat[dat['k'].isin(k_list)]\n",
    "#         dat = dat[dat['r'].isin(r_list)]\n",
    "#         dat.index = range(len(dat))\n",
    "#         all_dat.append(dat)\n",
    "        \n",
    "#         if save_path1:\n",
    "#             dat.to_csv(join(save_path1, basename(file)), index=False)\n",
    "#         if len(wt) == 0:\n",
    "#             wt = dat.mag.values.reshape(1, -1)\n",
    "#         else:\n",
    "#             wt = np.concatenate((wt, dat.mag.values.reshape(1, -1)), axis=0)\n",
    "    \n",
    "#     var = np.var(wt, axis=0)\n",
    "#     index_list = np.where(var > sorted(var, reverse=True)[rate])[0]\n",
    "#     index_list_ = np.where(var <= sorted(var, reverse=True)[rate])[0]\n",
    "#     new_wt = wt[:, index_list]\n",
    "    \n",
    "#     if plot:\n",
    "#         plt.figure(figsize=(14, 6))\n",
    "#         plt.subplot(1, 2, 1)\n",
    "#         x, y = np.meshgrid(k_list, r_list)\n",
    "#         z = wt[plot, :].reshape((33, 40))\n",
    "#         plt.contourf(x, y, z, cmap=plt.cm.bwr, levels=40)\n",
    "#         plt.xlabel(r'Wavenumber k $(\\AA^{-1})$', fontsize=15)\n",
    "#         plt.ylabel(r'Radial distance R $(\\AA^{-1})$', fontsize=12)\n",
    "#         plt.title(\"Wavelet Transform: Magnitude\", fontsize=15)\n",
    "        \n",
    "#         plt.subplot(1, 2, 2)\n",
    "#         x, y = np.meshgrid(k_list, r_list)\n",
    "#         z = wt[plot, :]\n",
    "#         z[[index_list_]] = 0\n",
    "#         z = z.reshape((33, 40))\n",
    "#         plt.contourf(x, y, z, cmap=plt.cm.bwr, levels=40)\n",
    "#         plt.xlabel(r'Wavenumber k $(\\AA^{-1})$', fontsize=15)\n",
    "#         plt.ylabel(r'Radial distance R $(\\AA^{-1})$', fontsize=12)\n",
    "#         plt.title(\"Wavelet Transform: Magnitude\", fontsize=15)\n",
    "        \n",
    "#         plt.show()\n",
    "#     if save_path2:\n",
    "#         for file, dat in zip(file_list, all_dat):\n",
    "#             dat = dat.loc[index_list, :]\n",
    "#             dat.index = range(len(dat))\n",
    "#             dat.to_csv(join(save_path2, basename(file)), index=False)\n",
    "#     return new_wt\n",
    "# #Function to read wavelet change image\n",
    "# class get_wt_pic(object):\n",
    "#     def __init__(self, path, label, img_size=(256, 256), n=-1):\n",
    "#         super().__init__()\n",
    "#         self.file_list = sorted(glob.glob(join(path, \"*.png\")), key=lambda x: int(splitext(basename(x))[0]))[:n]\n",
    "#         self.label = label\n",
    "#         self.img_size = img_size\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         pic = Image.open(self.file_list[index])\n",
    "#         pic = transforms.Resize(self.img_size)(pic)  \n",
    "#         pic = transforms.ToTensor()(pic)\n",
    "#         label = torch.Tensor([self.label[index]])\n",
    "#         return pic[:3, :, :], label\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.file_list)\n",
    "# Function to read label data\n",
    "def load_data(datadir, label_cn_path, label_cr_path, excluded_indices, min_label=0, max_label=np.inf):\n",
    "    # Read label files\n",
    "    label_cn_df = pd.read_csv(label_cn_path)\n",
    "    label_cr_df = pd.read_csv(label_cr_path)\n",
    "\n",
    "    # Filter out excluded samples\n",
    "    label_cn_df = label_cn_df[~label_cn_df['index'].isin(excluded_indices)]\n",
    "    label_cr_df = label_cr_df[~label_cr_df['index'].isin(excluded_indices)]\n",
    "\n",
    "    # Sort by index\n",
    "    label_cn_df.sort_values('index', inplace=True)\n",
    "    label_cr_df.sort_values('index', inplace=True)\n",
    "\n",
    "    # Extract label values\n",
    "    label_cn = label_cn_df.iloc[:, 1].values\n",
    "    label_cr = label_cr_df.iloc[:, 1].values\n",
    "\n",
    "    # Get file list and sort by the index in the filename\n",
    "    filelist = glob.glob(join(datadir, '*.csv'))\n",
    "    if not filelist:\n",
    "        print(f\"No files found in {datadir}\")\n",
    "        return None, None, None\n",
    "    filelist = sorted(filelist, key=lambda x: int(splitext(basename(x))[0]))\n",
    "\n",
    "    data_list = []\n",
    "    min_length = float('inf')\n",
    "    for file in filelist:\n",
    "        index = int(splitext(basename(file))[0])\n",
    "        if index in excluded_indices:\n",
    "            continue\n",
    "        temp = pd.read_csv(file)\n",
    "        temp_values = temp.iloc[:, 1].values  # Assuming data is in the second column\n",
    "        min_length = min(min_length, len(temp_values))  # Find the shortest length\n",
    "        data_list.append(temp_values)\n",
    "    \n",
    "    # Ensure all data lengths are consistent\n",
    "    data_list = [data[:min_length] for data in data_list]\n",
    "    data = np.vstack(data_list)\n",
    "    \n",
    "    # Ensure label lengths match the data\n",
    "    min_length = min(len(data), len(label_cn), len(label_cr))\n",
    "    data = data[:min_length]\n",
    "    label_cn = label_cn[:min_length]\n",
    "    label_cr = label_cr[:min_length]\n",
    "\n",
    "    # Filter labels\n",
    "    index = np.where((label_cn >= min_label) & (label_cn <= max_label))[0]\n",
    "    \n",
    "    return data[index], label_cn[index], label_cr[index]\n",
    "\n",
    "#Function to divide the data set\n",
    "def split_data(data, label_cn, label_cr, train_ratio=0.7, valid_ratio=0.2, test_ratio=0.1, random_seed=None):\n",
    "    assert math.isclose(train_ratio + valid_ratio + test_ratio, 1.0), \"The sum of the ratios must be 1\"\n",
    "    total_size = len(data)\n",
    "    train_size = int(total_size * train_ratio)\n",
    "    valid_size = int(total_size * valid_ratio)\n",
    "    test_size = total_size - train_size - valid_size\n",
    "\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    indices = np.arange(total_size)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    train_indices = indices[:train_size]\n",
    "    valid_indices = indices[train_size:train_size + valid_size]\n",
    "    test_indices = indices[train_size + valid_size:]\n",
    "\n",
    "    train_data, valid_data, test_data = data[train_indices], data[valid_indices], data[test_indices]\n",
    "    train_label_cn, valid_label_cn, test_label_cn = label_cn[train_indices], label_cn[valid_indices], label_cn[test_indices]\n",
    "    train_label_cr, valid_label_cr, test_label_cr = label_cr[train_indices], label_cr[valid_indices], label_cr[test_indices]\n",
    "\n",
    "    return (train_data, train_label_cn, train_label_cr, train_indices), \\\n",
    "           (valid_data, valid_label_cn, valid_label_cr, valid_indices), \\\n",
    "           (test_data, test_label_cn, test_label_cr, test_indices)\n",
    "#Function to save data\n",
    "def save_dataset(data_np_array, label_cn_np_array, label_cr_np_array, path_data, feature, suffix, method, indices, set_type):\n",
    "    file_feature = f\"{feature}_{set_type}_{method}{suffix}\"\n",
    "    path_file_feature = os.path.join(path_data, file_feature)\n",
    "    \n",
    "    with open(path_file_feature, 'w') as fout:\n",
    "        for i in indices:\n",
    "            if i >= data_np_array.shape[0]:\n",
    "                continue\n",
    "            for j in range(data_np_array.shape[1]):\n",
    "                fout.write(f\"{data_np_array[i, j]:14.6e}\")\n",
    "            fout.write('\\n')\n",
    "\n",
    "    file_label_cn = f\"label_cn_{set_type}_{method}{suffix}\"\n",
    "    path_file_label_cn = os.path.join(path_data, file_label_cn)\n",
    "    with open(path_file_label_cn, 'w') as fout:\n",
    "        for i in indices:\n",
    "            if i >= label_cn_np_array.shape[0]:\n",
    "                continue\n",
    "            fout.write(f\"{label_cn_np_array[i]:6.3f}\\n\")\n",
    "\n",
    "    file_label_cr = f\"label_cr_{set_type}_{method}{suffix}\"\n",
    "    path_file_label_cr = os.path.join(path_data, file_label_cr)\n",
    "    with open(path_file_label_cr, 'w') as fout:\n",
    "        for i in indices:\n",
    "            if i >= label_cr_np_array.shape[0]:\n",
    "                continue\n",
    "            fout.write(f\"{label_cr_np_array[i]:6.3f}\\n\")\n",
    "\n",
    "def save_all(data_np_array, label_cn_np_array, label_cr_np_array, path_data, feature, suffix, method, train_indices, valid_indices, test_indices):\n",
    "    if not os.path.exists(path_data):\n",
    "        os.makedirs(path_data)\n",
    "\n",
    "    save_dataset(data_np_array, label_cn_np_array, label_cr_np_array, path_data, feature, suffix, method, train_indices, 'train')\n",
    "    save_dataset(data_np_array, label_cn_np_array, label_cr_np_array, path_data, feature, suffix, method, valid_indices, 'valid')\n",
    "    save_dataset(data_np_array, label_cn_np_array, label_cr_np_array, path_data, feature, suffix, method, test_indices, 'test')\n",
    "\n",
    "    # Function to save the complete dataset\n",
    "    save_dataset(data_np_array, label_cn_np_array, label_cr_np_array, path_data, feature, suffix,f\"{method}\", np.arange(len(data_np_array)),'all')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2ac891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.959067Z",
     "start_time": "2025-01-21T02:00:10.935576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_cn finished\n",
      "label_cr finished\n"
     ]
    }
   ],
   "source": [
    "label_cn = pd.read_csv(cn_label_path)[:n]\n",
    "label_cr = pd.read_csv(cr_label_path)[:n]\n",
    "label_cn.to_csv(save_label_cn, index=False)\n",
    "print(f\"label_cn finished\")\n",
    "label_cr.to_csv(save_label_cr, index=False)\n",
    "print(f\"label_cr finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94141aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:10.962418Z",
     "start_time": "2025-01-21T02:00:10.960111Z"
    }
   },
   "outputs": [],
   "source": [
    "# def process_data(features,n):\n",
    "#     if 'chi' in features:\n",
    "#         chi = get_chi(chi_path, save_path=save_chi_path, n=n, plot=1)\n",
    "#         print(f\"fature_chi finished\")\n",
    "#     if 'xmu' in features:\n",
    "#         xmu = get_xmu(xmu_path, save_path=save_xmu_path, n=n, plot=1)\n",
    "#         print(f\"fature_xmu finished\")\n",
    "#     if 'norm' in features:\n",
    "#         norm = get_norm(norm_path, save_path=save_norm_path, n=n, plot=1)\n",
    "#         print(f\"fature_norm finished\")\n",
    "#     if 'rdf' in features:\n",
    "#         rdf = get_rdf(rdf_path, save_path=save_rdf_path, n=n, plot=1)\n",
    "#         print(f\"fature_rdf finished\")\n",
    "#     if 'wt' in features:\n",
    "#         wt = get_wt(wt_path, n=n, save_path=save_wt_path, rate=150, plot=1)\n",
    "#         print(f\"fature_wt finished\")\n",
    "# process_data(features, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24de960b",
   "metadata": {},
   "source": [
    "#   Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67066e13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-21T02:00:35.246269Z",
     "start_time": "2025-01-21T02:00:10.963868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing feature: chi from 0926-datasets/prepare/chi\n",
      "Data shape: (4950, 400), Label CN shape: (4950,), Label CR shape: (4950,)\n",
      "Train data shape: (3465, 400), Train label CN shape: (3465,), Train label CR shape: (3465,)\n",
      "\n",
      "Valid data shape: (990, 400), Valid label CN shape: (990,), Valid label CR shape: (990,)\n",
      "\n",
      "Test data shape: (495, 400), Test label CN shape: (495,), Test label CR shape: (495,)\n",
      "\n",
      "Processing feature: xmu from 0926-datasets/prepare/xmu\n",
      "Data shape: (4950, 1000), Label CN shape: (4950,), Label CR shape: (4950,)\n",
      "Train data shape: (3465, 1000), Train label CN shape: (3465,), Train label CR shape: (3465,)\n",
      "\n",
      "Valid data shape: (990, 1000), Valid label CN shape: (990,), Valid label CR shape: (990,)\n",
      "\n",
      "Test data shape: (495, 1000), Test label CN shape: (495,), Test label CR shape: (495,)\n",
      "\n",
      "Processing feature: rdf from 0926-datasets/prepare/rdf\n",
      "Data shape: (4950, 100), Label CN shape: (4950,), Label CR shape: (4950,)\n",
      "Train data shape: (3465, 100), Train label CN shape: (3465,), Train label CR shape: (3465,)\n",
      "\n",
      "Valid data shape: (990, 100), Valid label CN shape: (990,), Valid label CR shape: (990,)\n",
      "\n",
      "Test data shape: (495, 100), Test label CN shape: (495,), Test label CR shape: (495,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process feature data and append log information\n",
    "for feature in features:\n",
    "    datadir = os.path.join(load_path, feature)\n",
    "    print(f\"Processing feature: {feature} from {datadir}\")\n",
    "    \n",
    "    # Load data\n",
    "    data, label_cn, label_cr = load_data(datadir, save_label_cn, save_label_cr, excluded_indices, min_label=0)\n",
    "    \n",
    "    if data is not None and label_cn is not None and label_cr is not None:\n",
    "\n",
    "        # Split and save data\n",
    "        (train_data, train_label_cn, train_label_cr, train_indices), \\\n",
    "        (valid_data, valid_label_cn, valid_label_cr, valid_indices), \\\n",
    "        (test_data, test_label_cn, test_label_cr, test_indices) = split_data(\n",
    "            data, label_cn, label_cr, train_ratio=train_set, valid_ratio=valid_set, test_ratio=test_set, random_seed=42\n",
    "        )\n",
    "        \n",
    "        save_all(data, label_cn, label_cr, save_data_path, feature, suffix, method, train_indices, valid_indices, test_indices)\n",
    "        print(f\"Data shape: {data.shape}, Label CN shape: {label_cn.shape}, Label CR shape: {label_cr.shape}\")\n",
    "        print(f\"Train data shape: {train_data.shape}, Train label CN shape: {train_label_cn.shape}, Train label CR shape: {train_label_cr.shape}\\n\")\n",
    "        print(f\"Valid data shape: {valid_data.shape}, Valid label CN shape: {valid_label_cn.shape}, Valid label CR shape: {valid_label_cr.shape}\\n\")\n",
    "        print(f\"Test data shape: {test_data.shape}, Test label CN shape: {test_label_cn.shape}, Test label CR shape: {test_label_cr.shape}\\n\")\n",
    "        \n",
    "        # Append log information\n",
    "        with open(join(load_path, \"out_log.txt\"), \"a\") as f:\n",
    "            f.write(f\"\\nProcessing feature: {feature}\\n\")\n",
    "            f.write(f\"Data shape: {data.shape}, Label CN shape: {label_cn.shape}, Label CR shape: {label_cr.shape}\\n\")\n",
    "            f.write(f\"Train data shape: {train_data.shape}, Train label CN shape: {train_label_cn.shape}, Train label CR shape: {train_label_cr.shape}\\n\")\n",
    "            f.write(f\"Valid data shape: {valid_data.shape}, Valid label CN shape: {valid_label_cn.shape}, Valid label CR shape: {valid_label_cr.shape}\\n\")\n",
    "            f.write(f\"Test data shape: {test_data.shape}, Test label CN shape: {test_label_cn.shape}, Test label CR shape: {test_label_cr.shape}\\n\")\n",
    "            f.write(\"Data split and saved successfully.\\n\")\n",
    "    else:\n",
    "        print(f\"Failed to load data for feature: {feature}\")\n",
    "        with open(join(prepare_path, \"out_log.txt\"), \"a\") as f:\n",
    "            f.write(f\"\\nFailed to load data for feature: {feature}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621fd06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
